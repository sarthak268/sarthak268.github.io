---
layout:     page
title:
permalink:  /
---

<div class="row">
    <div class="col-sm-6 col-xs-12">
        <img src="/img/cover2.jpg">
    </div>
    <div class="col-sm-6 col-xs-12" style="margin-bottom: 0;">
        Master's in Robotics<br>
        Carnegie Mellon University<br>
        sarthakb@andrew.cmu.edu
    </div>
</div>
<hr>

<a name="/news"></a>

# News
<!-- 
- [Feb 22] Runner-up for the [2020 AAAI/ACM SIGAI Doctoral Dissertation Award][aaai-dissertation-award].
- [Mar 21] Awarded the [Georgia Tech Sigma Xi Best PhD Thesis Award][sigma-xi-thesis-award].
- [Mar 21] Awarded the [Georgia Tech College of Computing Dissertation Award][coc-dissertation-award].
- [Nov 20] The [Open Catalyst Project][ocp] was covered by [Fortune][ocp-fortune], [Engadget][ocp-engadget], [CNBC][ocp-cnbc], [VentureBeat][ocp-venturebeat].
- [Nov 20] Organizing the 4th [Visually-Grounded Interaction & Language Workshop at NAACL][vigil20].
- [July 20] Presenting [Probing Emergent Semantics in Predictive Agents](#/qa-probing) at ICML 2020 ([Video][qa-probing-icml20-talk]).
- [Mar 20] I completed my PhD! My thesis, "Building agents that can see, talk, and act", is [here][thesis-pdf].
- [Nov 19] Organizing the [Visual Question Answering and Dialog workshop at CVPR 2020][51].
- [Sep 19] Organizing the [Visually-Grounded Interaction & Language Workshop at NeurIPS][vigil19].
- [Jun 19] Presenting [Targeted Multi-Agent Communication](#/multi-agent-comm) as an oral at ICML 2019 ([Video][tarmac-icml-talk]).
- [Mar 19] Co-founded [Caliper][caliper]. Caliper helps recruiters evaluate practical AI skills.
- [Feb 19] My work was featured in this [wonderful article by Georgia Tech][ic-gt-article].
- [Jan 19] Awarded the [Facebook Graduate Fellowship][fb-fellow-page].
- [Jan 19] Awarded the Microsoft Research PhD Fellowship (declined).
- [Jan 19] Awarded the NVIDIA Graduate Fellowship (declined).
- [Jan 19] Organizing the [2nd Visual Dialog Challenge][visdial-challenge-2].
- [Oct 18] Presenting [Neural Modular Control for Embodied QA](#/eqa-modular) at CoRL 2018 ([Video][58]).
- [Sep 18] Presenting [results and analysis of the 1st Visual Dialog Challenge][57] at ECCV 2018.
- [Jul 18] Presenting a [tutorial on Connecting Language and Vision to Actions][49] at [ACL 2018][50].
- [Jun 18] Organizing the 1st [Visual Dialog Challenge][53].
- [Jun 18] Presenting [Embodied Question Answering](#/embodied-qa) as an oral at CVPR 2018 ([Video][54]).
- [Jun 18] Organizing the [VQA Challenge and Visual Dialog Workshop at CVPR 2018][51].
- [Mar 18] Speaking on [Embodied Question Answering][40] at [NVIDIA GTC][42] ([Video][52]).
- [Dec 17] Awarded the [Adobe Research Fellowship][39]. ([Department's news story][44])
- [Dec 17] Awarded the [Snap Inc. Research Fellowship][36]. ([Department's news story][43])
- [Oct 17] Presenting [Cooperative Visual Dialog Agents](#/visdial-rl) as an oral at ICCV 2017 ([Video][37]).
- [Jul 17] Presenting [Visual Dialog](//visualdialog.org) at the [VQA Challenge Workshop](http://visualqa.org/workshop.html), CVPR 2017 ([Video][41]).
- [Jul 17] Presenting our paper on [Visual Dialog](#/visdial) as a spotlight at CVPR 2017 ([Video][38]). -->

- [May 23] Selected among 200 young researchers to attend the HLF 2023.
- [May 23] Work on Visual Concept Learning accepted in CoLLAs 2023.
- [Sept 22] Joined AART Lab, CMU -- working on concept learning using domain knowledge priors. 
- [Aug 22] Joined Master's in Robotics Program at the Robotics Institute, CMU.
- [Dec 22] Work on talking face generation won the Best Demo Paper award at ACM MM Asia.
- [Jan 22] Joined Preimage as a Deep Learning Research Engineer -- working on Sparse-view 3D Reconstruction.
- [Oct 20] Joined CLVR Lab, USC as a Visiting Researcher -- working on continual skill learning and OOD adaptation in robotics.
- [Aug 20] Joined DeCLaRe Lab, SUTD as a Research Assistant -- working on vision-language applications.
- [Aug 20] Graduated from IIIT-Delhi with Honors.

<div id="read-more-button">
    <a nohref>Read more</a>
</div>

<hr>

<a name="/bio"></a>

# Bio

<!-- I am a Research Scientist at Fundamental AI Research (FAIR) at Meta AI working on deep neural
networks and its applications in climate change. My current focus is on
electrocatalyst discovery for renewable energy storage as part of the [Open Catalyst Project][ocp].
Renewable energy sources (like solar, wind) are great but intermittent -- the sun
shines only during the day. During the evening, we fall back on fossil fuels.
To avoid this, we need to discover cheap, scalable ways of converting electricity
from renewable sources to storable forms, so that we can transfer it from times of
peak generation to peak demand.
AI can help accelerate the chemical simulations needed to make these discoveries.

Before this, I was a Computer Science PhD student at Georgia Tech, advised by [Dhruv Batra][2],
and working closely with [Devi Parikh][3], where I focused on developing
artificial agents that can [<i>see</i> (computer vision), <i>talk</i> (language modeling), and <i>act</i> (reinforcement learning)][thesis-pdf]. -->

I am a graduate student at [Robotics Institute](https://www.ri.cmu.edu/), [Carnegie Mellon University](https://www.cmu.edu/). I am working at [Advanced Agents - Robotics Technology Lab](https://www.ri.cmu.edu/robotics-groups/advanced-agent-robotics-technology-lab/) advised by of [Dr. Katia Sycara](https://en.wikipedia.org/wiki/Katia_Sycara).

Most recently, I worked as a Deep Learning Engineer at [Preimage](https://preimage.ai/), where I worked on problems involving 3D vision for drone-based photogrammetry.

Previously, I worked as a Visiting Researcher at [Cognitive Learning and Vision for Robotics (CLVR) Lab](https://www.clvrai.com/), [University of Southern California (USC), USA](https://www.usc.edu/) under the guidance of [Dr. Joseph Lim](https://viterbi-web.usc.edu/~limjj/). I majorly focus on problems involving adaptation of policies to novel unseen environments.

During my short stint as a Research Assistant at the [Deep Cognition and Language Research Lab](https://declare-lab.net/index), [Singapore University of Technology and Design, Singapore](https://www.sutd.edu.sg/), advised by [Dr. Soujanya Poria](https://sporia.info/index), I worked on zero-shot visual classification and vision & language applications. 

I completed my B.Tech (with Honors) in ECE from [IIIT-Delhi](https://www.iiitd.ac.in/). In my Bachelor's Thesis under [Dr. Saket Anand](http://faculty.iiitd.ac.in/~anands/) in collaboration with [Dr. Pavan Turaga, Geometric Media Lab](https://pavanturaga.com/) (Arizona State Univesity, USA) at the [Infosys Center of Artificial Intelligence](https://cai.iiitd.ac.in/), I worked on unsupervised representation learning for disentangling multiple factors of variation in images. 

Previously, I worked as an intern at the [SUTD Brain Lab](https://sutdbrain.wordpress.com/about/) under [Dr. Nengli Lim](https://www.linkedin.com/in/nengli-lim-48a22b14/?originalSubdomain=sg) on the disentanglement of video sequences using Gaussian processes. I have also worked at [Collaborative Robotics Lab (CORAL)](http://robotics.iiitd.edu.in/coral/?page_id=20) and [Multi-robot Autonomy (MOON) Lab](https://sites.google.com/view/m00nlab/home?authuser=0) under [Dr. P.B. Sujit](https://eecs.iiserb.ac.in/faculty_profile.php?id=OQ==&lname=c3VqaXQ=) on target tracking using deep reinforcement learning.

I am mainly interested in unsupervised (or self-supervised) representation learning for visual data, vision for robotics, robot learning.
Reach out to me at: [sarthakb@andrew.cmu.edu](mailto: sarthakb@andrew.cmu.edu).

**_If you are interested in research collaboration, please drop me an email._**

<div class="row" id="timeline-logos">
    <div class="col-xs-3">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a href="//"><img src="/img/logos/iiitd.png"></a>
        </div>
        <div class="logo-desc">
            IIIT Delhi<br>
            2016 - 2020
        </div>
    </div>
    <div class="col-xs-3">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a href="//"><img style="width:120px;" src="/img/logos/sutd.png"></a>
        </div>
        <div class="logo-desc">
            Singapore University of Technology and Design<br>
            Summer 2019, Summer 2020
        </div>
    </div>
    <div class="col-xs-3">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a href="//"><img src="/img/logos/usc.png"></a>
        </div>
        <div class="logo-desc">
            University of Southern California<br>
            2020 - 2021
        </div>
    </div>
    <div class="col-xs-2">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a target="_blank" href="//"><img src="/img/logos/preimage.jpg"></a>
        </div>
        <div class="logo-desc">
            Preimage<br>
            2021
        </div>
    </div>
    <div class="col-xs-3">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a target="_blank" href="//"><img style="width:120px;" src="/img/logos/cmu.jpg"></a>
        </div>
        <div class="logo-desc">
            Carnegie Mellon University<br>
            2022-
        </div>
    </div>
    <!-- <div class="col-xs-3">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a target="_blank" href="//www.tesla.com/autopilotAI"><img style="width:120px;" src="/img/logos/tesla.jpg"></a>
        </div>
        <div class="logo-desc">
            Tesla Autopilot<br>
            Summer 2019
        </div>
    </div>
    <div class="col-xs-3">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a target="_blank" href="//research.fb.com/category/facebook-ai-research/"><img style="width:160px;" src="/img/logos/fair3.png"></a>
        </div>
        <div class="logo-desc">
            Facebook AI Research<br>
            Present
        </div>
    </div> -->
</div>

<!-- During my PhD, I interned thrice at Facebook AI Research — Summer 2017 and Spring 2018
at Menlo Park, working with [Georgia Gkioxari][46],
[Devi Parikh][47] and [Dhruv Batra][48] on training embodied agents for navigation and
question-answering in simulated environments (see [embodiedqa.org][40]), and Summer
2018 at Montréal, working with [Mike Rabbat][55] and [Joelle Pineau][56] on
<a target="_blank" href="https://arxiv.org/abs/1810.11187">communication protocols in multi-agent reinforcement learning</a>.
In 2019, I interned at DeepMind in London working on grounded language learning
with [Felix Hill][felix-hill], [Laura Rimell][laura-rimell],
and [Stephen Clark][stephen-clark], and at Tesla Autopilot in Palo Alto working on
differentiable neural architecture search with [Andrej Karpathy][andrej-karpathy].

My PhD research was supported by fellowships from [Facebook][fb-fellow-page],
[Adobe][39], and [Snap][36].

Prior to joining grad school, I worked on neural coding in zebrafish tectum
as an intern under [Prof. Geoffrey Goodhill][4] and [Lilach Avitan][5]
at the [Goodhill Lab][6], Queensland Brain Institute.

I got my Bachelor's at [IIT Roorkee][31] in 2015.
During my undergrad, I took part in
Google Summer of Code ([2013][8] and [2014][9]),
won several competitions ([Yahoo! HackU!][10],
[Microsoft Code.Fun.Do.][11], Deloitte CCTC [2013][12] and [2014][13]),
and owe most of my programming/tinkering bent to [SDSLabs][16].

On the side, I built [aideadlin.es][34] (countdowns to a bunch of CV/NLP/ML/AI conference deadlines)
and [aipaygrad.es][aipaygrad.es] (statistics of industry job offers in AI),
[neural-vqa][19] and its extension [neural-vqa-attention][35],
[HackFlowy][20], [graf][21], [Erdős][17], [etc][22].
I also occasionally dabble in [generative art](/art).
I like [this map][conquerearth] tracking the places I've been to.
[Blog posts from a previous life](/archive). -->

---

<a name="/publications"></a>

# Conference Publications

<a name="/relate"></a>
<h2 class="pubt">Sample-Efficient Learning of Novel Visual Concepts</h2>
<p class="pubd">
    <span class="authors">S. Bhagat*, S. Stepputtis*, J. Campbell, K. Sycara</span><br>
    <span class="conf">Conference on Lifelong Learning Agents (CoLLAs), 2023</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2301.07302">Paper</a>
        <a target="_blank" href="https://github.com/Ram81/pirlnav">Code</a>
        <!-- <a target="_blank" href="https://ram81.github.io/projects/pirlnav.html">Website</a> -->
    </span>
</p>
<!-- <img src="/img/habitat/pirlnav.png"> -->
<hr>

<!-- <a name="/oc22"></a>
<h2 class="pubt">The Open Catalyst 2022 (OC22) Dataset and Challenges for Oxide Electrocatalysis</h2>
<p class="pubd">
    <span class="authors">Richard Tran*, Janice Lan*, Muhammed Shuaibi*, Siddharth Goyal*, Brandon M. Wood*, Abhishek Das, Javier Heras-Domingo, Adeesh Kolluru, Ammar Rizvi, Nima Shoghi, Anuroop Sriram, Zachary Ulissi, C. Lawrence Zitnick</span><br>
    <span class="conf"></span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2206.08917">Paper</a>
        <a target="_blank" href="https://github.com/open-catalyst-project/ocp">Code</a>
        <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/blob/main/DATASET.md#open-catalyst-2022-oc22">Dataset</a>
    </span>
    <div class="row pressdiv" style="margin: 5px 0 0 0; line-height: 1.4em;">
        <a style="border-bottom: 0;" target="_blank" href="https://ai.facebook.com/blog/accelerating-renewable-energy-with-a-new-data-set-for-green-hydrogen-fuel/">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/fair2.png" style="background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"... new dataset for green hydrogen fuel" by Janice, Siddharth, Ammar, Larry</span>
            </div>
        </a>
    </div>
</p>
<img src="/img/ocp/oc22.jpg">
<hr> -->

<a name="/faircop"></a>
<h2 class="pubt">Suspect Identification Framework using Contrastive Relevance Feedback</h2>
<p class="pubd">
    <span class="authors">D. Gupta, A. Saini, D. Bhasin, S. Bhagat, S. Uppal, P. Kumaraguru, R. Shah</span><br>
    <span class="conf">Winter Conference on Applications of Computer Vision (WACV), 2023</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2204.02782">Paper</a>
        <!-- <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/tree/main/ocpmodels/models/gemnet_oc">Code</a> -->
    </span>
</p>
<!-- <img src="/img/ocp/gemnet_oc.jpg"> -->
<hr>

<a name="/emogen"></a>
<h2 class="pubt">Emotional Talking Faces: Making Videos More Expressive and Realistic</h2>
<p class="pubd">
    <span class="authors">S. Goyal, S. Uppal, S. Bhagat, D. Goel, S. Mali, Y. Yu, Y. Yin, R. Shah</span><br>
    <span class="conf">ACM Multimedia Asia (MMAsia), 2022 (Won Best Demo Paper Award)</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2206.14331">Paper</a>
        <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/tree/main/ocpmodels/models/scn">Code</a>
    </span>
</p>
<!-- <img src="/img/ocp/scn.jpg"> -->
<hr>

<a name="/mgpvae"></a>
<h2 class="pubt">Disentangling Multiple Features in Video Sequences using Gaussian Processes in Variational Autoencoders</h2>
<p class="pubd">
    <span class="authors">S. Bhagat*, S. Uppal*, Z. Yin, N. Lim</span><br>
    <span class="conf">European Conference on Computer Vision (ECCV), 2020</span>
    <span class="conf">ICVGIP 2020 Vision India</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2206.14331">Paper</a>
        <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/tree/main/ocpmodels/models/scn">Code</a>
    </span>
</p>
<!-- <img src="/img/ocp/scn.jpg"> -->
<hr>

<a name="/uav"></a>
<h2 class="pubt">UAV Target Tracking in Urban Environments Using Deep Reinforcement Learning</h2>
<p class="pubd">
    <span class="authors">S. Bhagat, P.B. Sujit</span><br>
    <span class="conf">he 2020 International Conference on Unmanned Aircraft Systems (ICUAS), 2020 (Oral)</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2206.14331">Paper</a>
        <!-- <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/tree/main/ocpmodels/models/scn">Code</a> -->
    </span>
</p>
<!-- <img src="/img/ocp/scn.jpg"> -->
<hr>

<a name="/c3vqg"></a>
<h2 class="pubt">C3VQG: Category Consistent Cyclic Visual Question Generation.</h2>
<p class="pubd">
    <span class="authors">S. Uppal*, A. Madan*, S. Bhagat*, Y. Yu, R. Shah</span><br>
    <span class="conf">ACM Multimedia Asia (MMAsia), 2020</span>
    <span class="conf">VQA and Dialogue Workshop, Computer Vision and Pattern Recognition (CVPR), 2020 (Spotlight)</span>    
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2206.14331">Paper</a>
        <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/tree/main/ocpmodels/models/scn">Code</a>
    </span>
</p>
<!-- <img src="/img/ocp/scn.jpg"> -->
<hr>

<a name="/prose"></a>
<h2 class="pubt">PrOSe: Product of Orthogonal Spheres Parameterization for Disentangled Representation Learning</h2>
<p class="pubd">
    <span class="authors">A. Shukla, S. Bhagat*, S. Uppal*, S. Anand, P. Turaga</span><br>
    <span class="conf">British Machine Vision Conference (BMVC), 2019</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2206.14331">Paper</a>
        <!-- <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/tree/main/ocpmodels/models/scn">Code</a> -->
    </span>
</p>
<!-- <img src="/img/ocp/scn.jpg"> -->
<hr>

<a name="/icvgip"></a>
<h2 class="pubt">Geometry of Deep Generative Models for Disentangled Representations</h2>
<p class="pubd">
    <span class="authors">A. Shukla, S. Uppal*, S. Bhagat*, S. Anand, P. Turaga</span><br>
    <span class="conf">Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP), 2018</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2206.14331">Paper</a>
        <!-- <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/tree/main/ocpmodels/models/scn">Code</a> -->
    </span>
</p>
<!-- <img src="/img/ocp/scn.jpg"> -->
<hr>

# Journal Publications

<a name="/infofus"></a>
<h2 class="pubt">Multimodal Research in Vision and Language: A Review of Current and Emerging Trends</h2>
<p class="pubd">
    <span class="authors">S. Uppal*, S. Bhagat*, D. Hazarika, N. Majumdar, S. Poria, R. Zimmermann, A. Zadeh</span><br>
    <span class="conf">Information Fusion Journal, 2021 (Impact Factor: 15.7)</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2206.14331">Paper</a>
        <!-- <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/tree/main/ocpmodels/models/scn">Code</a> -->
    </span>
</p>
<!-- <img src="/img/ocp/scn.jpg"> -->
<hr>

<a name="/mdpi"></a>
<h2 class="pubt">Deep Reinforcement Learning for Soft Robotic Applications: Brief Overview with Impending Challenges</h2>
<p class="pubd">
    <span class="authors">S. Bhagat*, H. Banerjee*, Z. Tse, H. Ren</span><br>
    <span class="conf">Robotics 2019, 8(1), 4</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2206.14331">Paper</a>
        <!-- <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/tree/main/ocpmodels/models/scn">Code</a> -->
    </span>
</p>
<!-- <img src="/img/ocp/scn.jpg"> -->
<hr>

# Workshop Publications


<a name="/contr"></a>
<h2 class="pubt">Contrastive Personalization Approach to Suspect Identification</h2>
<p class="pubd">
    <span class="authors">D. Gupta, D. Bhasin, S. Bhagat, S. Uppal, P. Kumaraguru, R. Shah</span><br>
    <span class="conf">AAAI Student Abstract 2021</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2206.14331">Paper</a>
        <!-- <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/tree/main/ocpmodels/models/scn">Code</a> -->
    </span>
</p>
<!-- <img src="/img/ocp/scn.jpg"> -->
<hr>


<a name="/discont"></a>
<h2 class="pubt">DisCont: Self-Supervised Visual Attribute Disentanglement using Context Vectors</h2>
<p class="pubd">
    <span class="authors">S. Bhagat*, V. Udandarao*, S. Uppal*, S. Anand</span><br>
    <span class="conf">European Conference on Computer Vision (ECCV) Workshops, 2020</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2206.14331">Paper</a>
        <!-- <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/tree/main/ocpmodels/models/scn">Code</a> -->
    </span>
</p>
<!-- <img src="/img/ocp/scn.jpg"> -->
<hr>

<!-- <div id="vimeo-embed">
    <iframe src="https://player.vimeo.com/video/193092429?byline=0&portrait=0&color=ffffff" width="640" height="360" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div> -->

<!-- <a name="/talks"></a>

# Talks

<div class="row">
    <div class="col-xs-6">
        <p class="talkd">
            <img src="/img/talks/visdial_rl_iccv17.jpg">
        </p>
    </div>
    <div class="col-xs-6">
        <p class="talkd">
            <img src="/img/talks/embodiedqa_cvpr18_4.jpg">
        </p>
    </div>
</div>
<div class="row">
    <div class="col-xs-12">
        <div class="talkt">
            <a target="_blank" href="https://slideslive.com/38928261/probing-emergent-semantics-in-predictive-agents-via-question-answering">
                ICML 2020: Probing Emergent Semantics in Predictive Agents via Question Answering
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://slideslive.com/38917625/tarmac-targeted-multiagent-communication">
                ICML 2019 Imitation, Intent, and Interaction Workshop:
                Targeted Multi-Agent Communication
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.facebook.com/icml.imls/videos/444326646299556/">
                ICML 2019 Oral: Targeted Multi-Agent Communication
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.youtube.com/watch?v=WxYBp3Xr_Nc">
                Allen Institute for Artificial Intelligence: "Towards Agents that can See, Talk, and Act"
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.youtube.com/watch?v=xoHvho-YRgs&t=7330">
                CoRL 2018 Spotlight: Neural Modular Control for Embodied Question Answering
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://youtu.be/gz2VoDrvX-A?t=1h19m58s">
                CVPR 2018 Oral: Embodied Question Answering
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="http://on-demand.gputechconf.com/gtc/2018/video/S8582/">
                NVIDIA GTC 2018
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.youtube.com/watch?v=R4hugGnNr7s">
                ICCV 2017 Oral: Learning Cooperative Visual Dialog Agents with Deep RL
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://youtu.be/KAlGWMJnWyc?t=26m56s">
                Visual Question Answering Challenge Workshop, CVPR 2017
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.youtube.com/watch?v=I9OlorMh7wU">
                CVPR 2017 Spotlight: Visual Dialog
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="http://techtalks.tv/talks/towards-transparent-visual-question-answering-systems/63026/">
                Visualization for Deep Learning Workshop, ICML 2016
            </a>
        </div>
    </div>
</div>
<hr>

<a name="/projects"></a>

# Side projects

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="http://aipaygrad.es">aipaygrad.es</a></h2>
        <p class="talkd">
            aipaygrad.es provides statistics of industry job offers in Artificial Intelligence (AI).
            All data is anonymous, cross-verified against offer letters and will
            hopefully reduce information asymmetry.
            <a target="_blank" href="http://aipaygrad.es"><img style="margin-top: 10px;" src="/img/projects/ai-paygrades.png"></a>
        </p>
    </div>
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="http://aideadlin.es">aideadlin.es</a></h2>
        <p class="talkd">
            aideadlin.es is a webpage to keep track of CV/NLP/ML/AI conference deadlines. It's hosted on GitHub, and countdowns are automatically updated via pull requests to the data file in the repo.
            <a target="_blank" href="http://aideadlin.es"><img style="margin-top: 10px;" src="/img/projects/ai-deadlines-1547012831.png"></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/neural-vqa-attention">neural-vqa-attention</a></h2>
        <p class="talkd">
            Torch implementation of an attention-based visual question answering model (Yang et al., CVPR16).
            The model looks at an image, reads a question, and comes up with an answer to the question and a heatmap of where it looked in the image to answer it.
            Some results <a href="https://computing.ece.vt.edu/~abhshkdz/neural-vqa-attention/figures/">here</a>.
            <a target="_blank" href="https://github.com/abhshkdz/neural-vqa-attention"><img class="project-img" src="/img/projects/neural-vqa-attention.jpg"></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/neural-vqa">neural-vqa</a></h2>
        <p class="talkd">
            neural-vqa is an efficient, GPU-based Torch implementation of the visual question answering model from the NIPS 2015 paper 'Exploring Models and Data for Image Question Answering' by Ren et al.
            <a target="_blank" href="https://github.com/abhshkdz/neural-vqa"><img src="/img/projects/neural-vqa.jpg"></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://erdos.sdslabs.co">Erdős</a></h2>
        <p class="talkd">
            Erdős by <a target="_blank" href="//sdslabs.co">SDSLabs</a> is a competitive math learning platform, similar in spirit to <a href="https://projecteuler.net/">Project Euler</a>, albeit more feature-packed (support for holding competitions, has a social layer) and prettier.
            <a target="_blank" href="https://erdos.sdslabs.co"><img style="margin-top:10px;" src="/img/projects/erdos.jpg"></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/graf">graf</a></h2>
        <p class="talkd">
            graf plots pretty git contribution bar graphs in the terminal.
            <code>gem install graf</code> to install.
            <a target="_blank" href="https://github.com/abhshkdz/graf"><img style="margin-top:10px;" src="/img/projects/graf.gif"></a>
        </p>
    </div>
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/HackFlowy">HackFlowy</a></h2>
        <p class="talkd">
            Clone of <a href="//workflowy.com">WorkFlowy.com</a>, a beautiful, list-based note-taking website that has a 500-item monthly limit on the free tier :-(. This project is an open-source clone of WorkFlowy. "Make lists. Not war." :-)
            <a target="_blank" href="https://github.com/abhshkdz/HackFlowy"><img style="margin-top:40px;" src="/img/projects/hackflowy.png"></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/AirMaps">AirMaps</a></h2>
        <p class="talkd">
            AirMaps was a fun hackathon project that lets users navigate through Google Earth with gestures and speech commands using a Kinect sensor. It was the <a target="_blank" href="https://blog.sdslabs.co/2014/02/code-fun-do">winning entry in Microsoft Code.Fun.Do</a>.
            <a target="_blank" href="https://github.com/abhshkdz/AirMaps"><img style="margin-top:10px;" src="/img/projects/airmaps.jpg"></a>
        </p>
    </div>
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/sdslabs/hackview">HackView</a></h2>
        <p class="talkd">
            Another fun hackathon-winning project built during Yahoo! HackU! 2012 that involves webRTC-based P2P video chat, and was faster than any other video chat provider (at the time, before Google launched Hangouts).
        </p>
    </div>
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/8tracks-downloader">8tracks-downloader</a></h2>
        <p class="talkd">
            Ugly-looking, but super-effective bash script for downloading entire playlists from 8tracks. (Still works as of 10/2016).
        </p>
    </div>
</div> -->

<script src="/js/jquery.min.js"></script>
<script type="text/javascript">
    $('ul:gt(0) li:gt(12)').hide();
    $('#read-more-button > a').click(function() {
        $('ul:gt(0) li:gt(12)').show();
        $('#read-more-button').hide();
    });
</script>

---
<!-- 
[1]: //mlp.cc.gatech.edu
[2]: ///www.cc.gatech.edu/~dbatra/
[3]: //www.cc.gatech.edu/~parikh/
[4]: //www.qbi.uq.edu.au/professor-geoffrey-goodhill
[5]: //researchers.uq.edu.au/researcher/2490
[6]: http://cns.qbi.uq.edu.au/
[7]: //developers.google.com/open-source/gsoc/
[8]: /posts/summer-of-code/
[9]: /posts/gsoc-reunion-2014/
[10]: //blog.sdslabs.co/2012/09/hacku
[11]: //blog.sdslabs.co/2014/02/code-fun-do
[12]: //www.facebook.com/SDSLabs/posts/527540147292475
[13]: /posts/deloitte-cctc-3/
[14]: /posts/google-india-community-summit/
[15]: //blog.sdslabs.co/2013/10/syntax-error-2013
[16]: //sdslabs.co/
[17]: //erdos.sdslabs.co/
[18]: //projecteuler.net/
[19]: //github.com/abhshkdz/neural-vqa
[20]: //github.com/abhshkdz/HackFlowy
[21]: //github.com/abhshkdz/graf
[22]: //github.com/abhshkdz
[23]: //twitter.com/abhshkdz
[24]: //instagram.com/abhshkdz
[25]: http://x.abhishekdas.com/
[26]: https://abhishekdas.com/vqa-hat/
[27]: http://arxiv.org/abs/1606.03556
[28]: https://www.newscientist.com/article/2095616-robot-eyes-and-humans-fix-on-different-things-to-decode-a-scene/
[29]: https://www.technologyreview.com/s/601819/ai-is-learning-to-see-the-world-but-not-the-way-humans-do/
[30]: http://www.theverge.com/2016/7/12/12158238/first-click-deep-learning-algorithmic-black-boxes
[31]: http://iitr.ac.in/
[32]: https://www.facebook.com/dhruv.batra.1253/posts/1783087161932290
[33]: https://drive.google.com/file/d/1nObeNzl-sTy8I5QN1Jv8wscebKLv-6RY/view?usp=sharing
[34]: http://aideadlin.es/
[35]: //github.com/abhshkdz/neural-vqa-attention
[36]: https://snapresearchfellowship.splashthat.com/
[37]: https://www.youtube.com/watch?v=R4hugGnNr7s
[38]: https://www.youtube.com/watch?v=I9OlorMh7wU
[39]: https://adoberesearch.ctlprojects.com/fellowship/previous-fellowship-award-winners/
[40]: https://embodiedqa.org/
[41]: https://youtu.be/KAlGWMJnWyc?t=26m56s
[42]: https://2018gputechconf.smarteventscloud.com/connect/sessionDetail.ww?SESSION_ID=152715
[43]: https://www.ic.gatech.edu/news/600684/three-ic-students-earn-snap-research-awards
[44]: https://www.ic.gatech.edu/news/601084/new-research-fellowships-offer-two-students-funding-access-adobes-creative-cloud
[45]: https://github.com/facebookresearch/House3D
[46]: https://gkioxari.github.io/
[47]: https://research.fb.com/people/parikh-devi/
[48]: https://research.fb.com/people/batra-dhruv/
[49]: https://lvatutorial.github.io/
[50]: http://acl2018.org/tutorials/#connecting-language-and-vis
[51]: http://visualqa.org/workshop.html
[52]: http://on-demand.gputechconf.com/gtc/2018/video/S8582/
[53]: https://visualdialog.org/challenge/2018
[54]: https://youtu.be/gz2VoDrvX-A?t=1h19m58s
[55]: https://research.fb.com/people/rabbat-mike/
[56]: https://www.cs.mcgill.ca/~jpineau/
[57]: https://visualdialog.org/challenge/2018#winners
[58]: https://www.youtube.com/watch?v=xoHvho-YRgs&t=7330
[fb-fellow-page]: https://research.fb.com/announcing-the-2019-facebook-fellows-and-emerging-scholars/
[joelle-corl18-talk-mention]: https://www.youtube.com/watch?v=FSsEqEJKo8A&t=3497
[visdial-challenge-2]: https://visualdialog.org/challenge/2019
[ic-gt-article]: https://www.ic.gatech.edu/news/617061/see-and-say-abhishek-das-working-provide-crucial-communication-tools-intelligent-agents
[caliper]: https://caliper.ai
[felix-hill]: https://fh295.github.io
[laura-rimell]: http://www.rimell.cc/laura/
[stephen-clark]: https://sites.google.com/site/stephenclark609/
[andrej-karpathy]: https://karpathy.ai/
[vigil19]: https://vigilworkshop.github.io/2019
[tarmac-icml-talk]: https://www.facebook.com/icml.imls/videos/444326646299556/
[mastodon]: https://mastodon.social/web/accounts/1011404
[conquerearth]: https://conquer.earth/abhshkdz
[qa-probing-icml20-talk]: https://slideslive.com/38928261/probing-emergent-semantics-in-predictive-agents-via-question-answering
[vigil20]: https://vigilworkshop.github.io
[ocp]: https://opencatalystproject.org
[ocp-cnbc]: https://www.cnbc.com/2020/10/14/facebook-to-use-ai-in-bid-to-improve-renewable-energy-storage.html
[ocp-engadget]: https://engadget.com/facebook-deploys-its-ai-to-find-green-energy-storage-solutions-130041147.html
[ocp-fortune]: https://fortune.com/2020/10/14/facebook-ai-open-catalyst-dataset-chemistry-renewable-energy/
[ocp-venturebeat]: https://venturebeat.com/2020/10/14/facebook-and-carnegie-mellon-launch-project-to-discover-better-ways-to-store-renewable-energy/
[aipaygrad.es]: https://aipaygrad.es
[sigma-xi-thesis-award]: https://cpb-us-w2.wpmucdn.com/sites.gatech.edu/dist/0/283/files/2021/03/2021-Sigma-Xi-Research-Award-Winners.final_.pdf
[coc-dissertation-award]: https://sites.gatech.edu/gtcomputingawards2021/graduate-student-awards/
[thesis-pdf]: https://drive.google.com/file/u/2/d/1b2Gonazl1Os0eLPV9frkucEqSuRroEvD/view?usp=sharing
[aaai-dissertation-award]: https://aaai.org/Awards/dissertation-award.php -->
