---
layout:     page
title:
permalink:  /
---

<div class="row">
    <div class="col-sm-6 col-xs-12">
        <img src="/img/cover2.jpg">
    </div>
    <div class="col-sm-6 col-xs-12" style="margin-bottom: 0;">
        Master's in Robotics<br>
        Carnegie Mellon University<br>
        sarthakb@andrew.cmu.edu
    </div>
</div>
<hr>

<a name="/news"></a>

# News

- [May 23] Selected among 200 young researchers to attend the [HLF 2023](https://www.heidelberg-laureate-forum.org/forum/10th-hlf-2023.html).
- [May 23] Work on Visual Concept Learning accepted in [CoLLAs 2023](https://lifelong-ml.cc/Conferences/2023/venue) accepted as one of only 12 oral presentation.
- [Dec 22] Work on talking face generation won the Best Demo Paper award at ACM MM Asia.
- [Sept 22] Joined AART Lab, CMU -- working on concept learning using domain knowledge priors and intent inference for human-robot collaboration. 
- [Aug 22] Joined Master's in Robotics Program at the [Robotics Institute, CMU](https://www.ri.cmu.edu/).
- [Jan 22] Joined [Preimage](https://www.preimage.ai/) as a Deep Learning Research Engineer -- working on Sparse-view 3D Reconstruction.
- [Oct 20] Joined [CLVR Lab](https://www.clvrai.com/), USC as a Visiting Researcher -- working on continual skill learning and OOD adaptation in robotics.
- [Aug 20] Joined [DeCLaRe Lab](https://declare-lab.net/), SUTD as a Research Assistant -- working on vision-language applications.
- [Aug 20] Graduated from [IIIT-Delhi](https://www.iiitd.edu.in/) with Honors.
- [Aug 19] Received Dean's Award for Excellent Academic Performance for 4 consecutive semesters.
- [Aug 19] Received Dean's Award for Innovation, Research, and Development for Bachelor's thesis.
- [May 19] Joined SUTD Brain Lab as a Visiting Researcher -- working on disentangling video sequences using Gaussian process priors.
- [May 19] Received [Indian National Academy of Engineering (INAE) Fellowship](https://www.inae.in/#) by IISc.
- [Aug 18] Placed 6th out of 70 International Teams: [AUVSI SUAS](https://suas-competition.org/), Maryland, USA.
- [Aug 16] Received Chairman's Merit Scholarshop (4 / 278)
- [May 15] Received KVPY Fellowship (awarded to top 0.3%)

<div id="read-more-button">
    <a nohref>Read more</a>
</div>

<hr>

<a name="/bio"></a>

# Bio

I am a graduate student at [Robotics Institute](https://www.ri.cmu.edu/), [Carnegie Mellon University](https://www.cmu.edu/). I am working at [Advanced Agents - Robotics Technology Lab](https://www.ri.cmu.edu/robotics-groups/advanced-agent-robotics-technology-lab/) advised by of [Dr. Katia Sycara](https://en.wikipedia.org/wiki/Katia_Sycara).

Most recently, I worked as a Deep Learning Engineer at [Preimage](https://preimage.ai/), where I worked on problems involving 3D vision for drone-based photogrammetry.

Previously, I worked as a Visiting Researcher at [Cognitive Learning and Vision for Robotics (CLVR) Lab](https://www.clvrai.com/), [University of Southern California (USC), USA](https://www.usc.edu/) under the guidance of [Dr. Joseph Lim](https://viterbi-web.usc.edu/~limjj/). I majorly focused on problems involving adaptation of policies to novel unseen environments and continual learning of skills.

During my short stint as a Research Assistant at the [Deep Cognition and Language Research Lab](https://declare-lab.net/index), [Singapore University of Technology and Design, Singapore](https://www.sutd.edu.sg/), advised by [Dr. Soujanya Poria](https://sporia.info/index), I worked on zero-shot visual classification and vision & language applications. 

I completed my B.Tech (with Honors) in ECE from [IIIT-Delhi](https://www.iiitd.ac.in/). In my Bachelor's Thesis under [Dr. Saket Anand](http://faculty.iiitd.ac.in/~anands/) in collaboration with [Dr. Pavan Turaga, Geometric Media Lab](https://pavanturaga.com/) (Arizona State Univesity, USA) at the [Infosys Center of Artificial Intelligence](https://cai.iiitd.ac.in/). Here, I worked on unsupervised representation learning for disentangling multiple factors of variation in images. 

Previously, I worked as an intern at the [SUTD Brain Lab](https://sutdbrain.wordpress.com/about/) under [Dr. Nengli Lim](https://www.linkedin.com/in/nengli-lim-48a22b14/?originalSubdomain=sg) on the disentanglement of video sequences using Gaussian processes. I have also worked at [Collaborative Robotics Lab (CORAL)](http://robotics.iiitd.edu.in/coral/?page_id=20) and [Multi-robot Autonomy (MOON) Lab](https://sites.google.com/view/m00nlab/home?authuser=0) under [Dr. P.B. Sujit](https://eecs.iiserb.ac.in/faculty_profile.php?id=OQ==&lname=c3VqaXQ=) on target tracking using deep reinforcement learning.

I am mainly interested in representation learning for visual data, computer vision for robotics, and robot learning.
Reach out to me at: [sarthakb@andrew.cmu.edu](mailto: sarthakb@andrew.cmu.edu).

<br>

**_If you are interested in research collaboration, please drop me an email._**

<br><br>

<div class="row" id="timeline-logos">
    <div class="col-xs-3">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a href="//"><img src="/img/logos/iiitd.png"></a>
        </div>
        <div class="logo-desc">
            IIIT Delhi<br>
            2016 - 2020
        </div>
    </div>
    <div class="col-xs-3">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a href="//"><img src="/img/logos/sutd.png"></a>
        </div>
        <div class="logo-desc">
            SUTD<br>
            Summer 2019, 2020
        </div>
    </div>
    <div class="col-xs-3">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a href="//"><img src="/img/logos/usc.png"></a>
        </div>
        <div class="logo-desc">
            University of Southern California<br>
            2020 - 2021
        </div>
    </div>
    <div class="col-xs-3">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a target="_blank" href="//"><img src="/img/logos/preimage.jpg"></a>
        </div>
        <div class="logo-desc">
            Preimage<br>
            2021
        </div>
    </div>
    <div class="col-xs-3">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a target="_blank" href="//"><img src="/img/logos/cmu.jpg"></a>
        </div>
        <div class="logo-desc">
            Carnegie Mellon University<br>
            2022-
        </div>
    </div>
    <!-- <div class="col-xs-3">
        <div class="logo-wrap">
            <span class="helper"></span>
            <a target="_blank" href="//"><img src="/img/logos/hlf.jpg"></a>
        </div>
        <div class="logo-desc">
            HLF<br>
            Young Researcher
        </div>
    </div> -->
</div>

<a name="/publications"></a>

# Conference Publications

<a name="/relate"></a>
<h2 class="pubt">Sample-Efficient Learning of Novel Visual Concepts</h2>
<p class="pubd">
    <span class="authors">S. Bhagat*, S. Stepputtis*, J. Campbell, K. Sycara</span><br>
    <span class="conf">Conference on Lifelong Learning Agents (CoLLAs), 2023 (Oral, Top 20% of Accepted Papers)</span>
    <span class="links">
        <a target="_blank" href="https://sarthak268.github.io/sample-efficient-visual-concept-learning/">Website</a>
        <a target="_blank" href="https://arxiv.org/abs/2306.09482">Paper</a>
        <a target="_blank" href="https://github.com/sarthak268/sample-efficient-visual-concept-learning">Code</a>
        <a target="_blank" href="">Video</a>
    </span>
</p>
<!-- <img src="/img/habitat/pirlnav.png"> -->
<hr>

<!-- <a name="/oc22"></a>
<h2 class="pubt">The Open Catalyst 2022 (OC22) Dataset and Challenges for Oxide Electrocatalysis</h2>
<p class="pubd">
    <span class="authors">Richard Tran*, Janice Lan*, Muhammed Shuaibi*, Siddharth Goyal*, Brandon M. Wood*, Abhishek Das, Javier Heras-Domingo, Adeesh Kolluru, Ammar Rizvi, Nima Shoghi, Anuroop Sriram, Zachary Ulissi, C. Lawrence Zitnick</span><br>
    <span class="conf"></span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2206.08917">Paper</a>
        <a target="_blank" href="https://github.com/open-catalyst-project/ocp">Code</a>
        <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/blob/main/DATASET.md#open-catalyst-2022-oc22">Dataset</a>
    </span>
    <div class="row pressdiv" style="margin: 5px 0 0 0; line-height: 1.4em;">
        <a style="border-bottom: 0;" target="_blank" href="https://ai.facebook.com/blog/accelerating-renewable-energy-with-a-new-data-set-for-green-hydrogen-fuel/">
            <div class="col-lg-1 col-md-1 col-xs-2" style="padding: 0;">
                <img src="/img/logos/fair2.png" style="background: white; width: 60px;">
            </div>
            <div class="col-lg-11 col-md-11 col-xs-10">
                <span class="presslink">"... new dataset for green hydrogen fuel" by Janice, Siddharth, Ammar, Larry</span>
            </div>
        </a>
    </div>
</p>
<img src="/img/ocp/oc22.jpg">
<hr> -->

<a name="/faircop"></a>
<h2 class="pubt">Suspect Identification Framework using Contrastive Relevance Feedback</h2>
<p class="pubd">
    <span class="authors">D. Gupta, A. Saini, D. Bhasin, S. Bhagat, S. Uppal, P. Kumaraguru, R. Shah</span><br>
    <span class="conf">Winter Conference on Applications of Computer Vision (WACV), 2023</span>
    <span class="links">
        <a target="_blank" href="https://openaccess.thecvf.com/content/WACV2023/papers/Gupta_A_Suspect_Identification_Framework_Using_Contrastive_Relevance_Feedback_WACV_2023_paper.pdf">Paper</a>
        <a target="_blank" href="https://youtu.be/Vf-nd2N5aYM">Video</a>
    </span>
</p>
<!-- <img src="/img/ocp/gemnet_oc.jpg"> -->
<hr>

<a name="/emogen"></a>
<h2 class="pubt">Emotional Talking Faces: Making Videos More Expressive and Realistic</h2>
<p class="pubd">
    <span class="authors">S. Goyal, S. Uppal, S. Bhagat, D. Goel, S. Mali, Y. Yu, Y. Yin, R. Shah</span><br>
    <span class="conf">ACM Multimedia Asia (MMAsia), 2022 (Won Best Demo Paper Award)</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/pdf/2303.11548.pdf">Paper</a>
        <a target="_blank" href="https://github.com/sahilg06/EmoGen">Code</a>
        <a target="_blank" href="https://youtu.be/bYPX0zp4MY4">Video</a>
        <a target="_blank" href="https://midas.iiitd.edu.in/emo/">Project Page</a>
    </span>
</p>
<!-- <img src="/img/ocp/scn.jpg"> -->
<hr>

<a name="/mgpvae"></a>
<h2 class="pubt">Disentangling Multiple Features in Video Sequences using Gaussian Processes in Variational Autoencoders</h2>
<p class="pubd">
    <span class="authors">S. Bhagat*, S. Uppal*, Z. Yin, N. Lim</span><br>
    <span class="conf">European Conference on Computer Vision (ECCV), 2020<br>
    ICVGIP 2020 Vision India</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/pdf/2001.02408.pdf">Paper</a>
        <a target="_blank" href="https://github.com/SUTDBrainLab/MGP-VAE">Code</a>
        <a target="_blank" href="https://youtu.be/35aGG1iZ_Vc">Video</a>
    </span>
</p>
<!-- <img src="/img/ocp/scn.jpg"> -->
<hr>

<a name="/uav"></a>
<h2 class="pubt">UAV Target Tracking in Urban Environments Using Deep Reinforcement Learning</h2>
<p class="pubd">
    <span class="authors">S. Bhagat, P.B. Sujit</span><br>
    <span class="conf">International Conference on Unmanned Aircraft Systems (ICUAS), 2020 (Oral)</span>
    <span class="links">
        <a target="_blank" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9213856">Paper</a>
        <a target="_blank" href="https://youtu.be/4hAEp5aE88A">Video</a>
    </span>
</p>
<!-- <img src="/img/ocp/scn.jpg"> -->
<hr>

<a name="/c3vqg"></a>
<h2 class="pubt">C3VQG: Category Consistent Cyclic Visual Question Generation.</h2>
<p class="pubd">
    <span class="authors">S. Uppal*, A. Madan*, S. Bhagat*, Y. Yu, R. Shah</span><br>
    <span class="conf">ACM Multimedia Asia (MMAsia), 2020<br>
    VQA and Dialogue Workshop, Computer Vision and Pattern Recognition (CVPR), 2020 (Spotlight)</span>    
    <span class="links">
        <a target="_blank" href="https://dl.acm.org/doi/pdf/10.1145/3444685.3446302">Paper</a>
        <a target="_blank" href="https://github.com/sarthak268/c3vqg-official">Code</a>
        <a target="_blank" href="https://youtu.be/SuTzKcIRGuw">Video</a>
    </span>
</p>
<!-- <img src="/img/ocp/scn.jpg"> -->
<hr>

<a name="/prose"></a>
<h2 class="pubt">PrOSe: Product of Orthogonal Spheres Parameterization for Disentangled Representation Learning</h2>
<p class="pubd">
    <span class="authors">A. Shukla, S. Bhagat*, S. Uppal*, S. Anand, P. Turaga</span><br>
    <span class="conf">British Machine Vision Conference (BMVC), 2019</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/pdf/1907.09554.pdf">Paper</a>
        <!-- <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/tree/main/ocpmodels/models/scn">Code</a> -->
    </span>
</p>
<!-- <img src="/img/ocp/scn.jpg"> -->
<hr>

<a name="/icvgip"></a>
<h2 class="pubt">Geometry of Deep Generative Models for Disentangled Representations</h2>
<p class="pubd">
    <span class="authors">A. Shukla, S. Uppal*, S. Bhagat*, S. Anand, P. Turaga</span><br>
    <span class="conf">Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP), 2018</span>
    <span class="links">
        <a target="_blank" href="https://dl.acm.org/doi/pdf/10.1145/3293353.3293422">Paper</a>
        <!-- <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/tree/main/ocpmodels/models/scn">Code</a> -->
    </span>
</p>
<!-- <img src="/img/ocp/scn.jpg"> -->
<hr>

# Journal Publications

<a name="/infofus"></a>
<h2 class="pubt">Multimodal Research in Vision and Language: A Review of Current and Emerging Trends</h2>
<p class="pubd">
    <span class="authors">S. Uppal*, S. Bhagat*, D. Hazarika, N. Majumdar, S. Poria, R. Zimmermann, A. Zadeh</span><br>
    <span class="conf">Information Fusion Journal, 2021 (Impact Factor: 15.7)</span>
    <span class="links">
        <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S1566253521001512">Paper</a>
        <!-- <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/tree/main/ocpmodels/models/scn">Code</a> -->
    </span>
</p>
<!-- <img src="/img/ocp/scn.jpg"> -->
<hr>

<a name="/mdpi"></a>
<h2 class="pubt">Deep Reinforcement Learning for Soft Robotic Applications: Brief Overview with Impending Challenges</h2>
<p class="pubd">
    <span class="authors">S. Bhagat*, H. Banerjee*, Z. Tse, H. Ren</span><br>
    <span class="conf">Robotics 2019, 8(1), 4</span>
    <span class="links">
        <a target="_blank" href="https://www.mdpi.com/2218-6581/8/1/4">Paper</a>
        <!-- <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/tree/main/ocpmodels/models/scn">Code</a> -->
    </span>
</p>
<!-- <img src="/img/ocp/scn.jpg"> -->
<hr>

# Thesis


<a name="/contr"></a>
<h2 class="pubt">Geometry of Neural Network based Disentangled Latent Space Models</h2>
<p class="pubd">
    <span class="authors">Advisors: S. Anand (IIIT-Delhi), P. Turaga (Arizona State University)</span><br>
    <span class="conf">Bachelor's Thesis</span>
    <span class="links">
        <a target="_blank" href="https://www.researchgate.net/profile/Sarthak-Bhagat/publication/346983991_Geometry_of_Neural_Network_based_Disentangled_Latent_Space_Models/links/5fd74b4445851553a0b59699/Geometry-of-Neural-Network-based-Disentangled-Latent-Space-Models.pdf">Thesis</a>
        <!-- <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/tree/main/ocpmodels/models/scn">Code</a> -->
    </span>
</p>
<!-- <img src="/img/ocp/scn.jpg"> -->
<hr>


# Workshop Publications

<!-- <a name="/contr"></a>
<h2 class="pubt">Sparse, Multi-Intent Inference for Action Prediction through Value Decomposition</h2>
<p class="pubd">
    <span class="authors">J. Campbell, S. Stepputtis, S. Bhagat, Y. Xie, K. Sycara</span><br>
    <span class="conf">Social Intelligence in Humans and Robots Workshop, RSS 2023</span>
    <span class="links">
        <a target="_blank" href="">Paper</a>
    </span>
</p>
<hr> -->


<a name="/contr"></a>
<h2 class="pubt">Contrastive Personalization Approach to Suspect Identification</h2>
<p class="pubd">
    <span class="authors">D. Gupta, D. Bhasin, S. Bhagat, S. Uppal, P. Kumaraguru, R. Shah</span><br>
    <span class="conf">AAAI Student Abstract 2021</span>
    <span class="links">
        <a target="_blank" href="https://ojs.aaai.org/index.php/AAAI/article/download/21617/21366">Paper</a>
        <!-- <a target="_blank" href="https://github.com/Open-Catalyst-Project/ocp/tree/main/ocpmodels/models/scn">Code</a> -->
    </span>
</p>
<!-- <img src="/img/ocp/scn.jpg"> -->
<hr>


<a name="/discont"></a>
<h2 class="pubt">DisCont: Self-Supervised Visual Attribute Disentanglement using Context Vectors</h2>
<p class="pubd">
    <span class="authors">S. Bhagat*, V. Udandarao*, S. Uppal*, S. Anand</span><br>
    <span class="conf">Perception Through Structured Generative Models, European Conference on Computer Vision (ECCV), 2020 <br>
    ML Interpretability for Scientific Discovery, International Conference on Machine Learning (ICML), 2020.</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/pdf/2006.05895.pdf">Paper</a>
        <a target="_blank" href="https://github.com/sarthak268/DisCont">Code</a>
        <a target="_blank" href="https://youtu.be/lIYCMHesVBw">Video</a>
    </span>
</p>
<!-- <img src="/img/ocp/scn.jpg"> -->
<hr>

<!-- <div id="vimeo-embed">
    <iframe src="https://player.vimeo.com/video/193092429?byline=0&portrait=0&color=ffffff" width="640" height="360" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</div> -->

<!-- <a name="/talks"></a>

# Talks

<div class="row">
    <div class="col-xs-6">
        <p class="talkd">
            <img src="/img/talks/visdial_rl_iccv17.jpg">
        </p>
    </div>
    <div class="col-xs-6">
        <p class="talkd">
            <img src="/img/talks/embodiedqa_cvpr18_4.jpg">
        </p>
    </div>
</div>
<div class="row">
    <div class="col-xs-12">
        <div class="talkt">
            <a target="_blank" href="https://slideslive.com/38928261/probing-emergent-semantics-in-predictive-agents-via-question-answering">
                ICML 2020: Probing Emergent Semantics in Predictive Agents via Question Answering
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://slideslive.com/38917625/tarmac-targeted-multiagent-communication">
                ICML 2019 Imitation, Intent, and Interaction Workshop:
                Targeted Multi-Agent Communication
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.facebook.com/icml.imls/videos/444326646299556/">
                ICML 2019 Oral: Targeted Multi-Agent Communication
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.youtube.com/watch?v=WxYBp3Xr_Nc">
                Allen Institute for Artificial Intelligence: "Towards Agents that can See, Talk, and Act"
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.youtube.com/watch?v=xoHvho-YRgs&t=7330">
                CoRL 2018 Spotlight: Neural Modular Control for Embodied Question Answering
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://youtu.be/gz2VoDrvX-A?t=1h19m58s">
                CVPR 2018 Oral: Embodied Question Answering
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="http://on-demand.gputechconf.com/gtc/2018/video/S8582/">
                NVIDIA GTC 2018
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.youtube.com/watch?v=R4hugGnNr7s">
                ICCV 2017 Oral: Learning Cooperative Visual Dialog Agents with Deep RL
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://youtu.be/KAlGWMJnWyc?t=26m56s">
                Visual Question Answering Challenge Workshop, CVPR 2017
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="https://www.youtube.com/watch?v=I9OlorMh7wU">
                CVPR 2017 Spotlight: Visual Dialog
            </a>
        </div>
        <div class="talkt">
            <a target="_blank" href="http://techtalks.tv/talks/towards-transparent-visual-question-answering-systems/63026/">
                Visualization for Deep Learning Workshop, ICML 2016
            </a>
        </div>
    </div>
</div>
<hr>

<a name="/projects"></a>

# Side projects

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="http://aipaygrad.es">aipaygrad.es</a></h2>
        <p class="talkd">
            aipaygrad.es provides statistics of industry job offers in Artificial Intelligence (AI).
            All data is anonymous, cross-verified against offer letters and will
            hopefully reduce information asymmetry.
            <a target="_blank" href="http://aipaygrad.es"><img style="margin-top: 10px;" src="/img/projects/ai-paygrades.png"></a>
        </p>
    </div>
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="http://aideadlin.es">aideadlin.es</a></h2>
        <p class="talkd">
            aideadlin.es is a webpage to keep track of CV/NLP/ML/AI conference deadlines. It's hosted on GitHub, and countdowns are automatically updated via pull requests to the data file in the repo.
            <a target="_blank" href="http://aideadlin.es"><img style="margin-top: 10px;" src="/img/projects/ai-deadlines-1547012831.png"></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/neural-vqa-attention">neural-vqa-attention</a></h2>
        <p class="talkd">
            Torch implementation of an attention-based visual question answering model (Yang et al., CVPR16).
            The model looks at an image, reads a question, and comes up with an answer to the question and a heatmap of where it looked in the image to answer it.
            Some results <a href="https://computing.ece.vt.edu/~abhshkdz/neural-vqa-attention/figures/">here</a>.
            <a target="_blank" href="https://github.com/abhshkdz/neural-vqa-attention"><img class="project-img" src="/img/projects/neural-vqa-attention.jpg"></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/neural-vqa">neural-vqa</a></h2>
        <p class="talkd">
            neural-vqa is an efficient, GPU-based Torch implementation of the visual question answering model from the NIPS 2015 paper 'Exploring Models and Data for Image Question Answering' by Ren et al.
            <a target="_blank" href="https://github.com/abhshkdz/neural-vqa"><img src="/img/projects/neural-vqa.jpg"></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://erdos.sdslabs.co">Erdős</a></h2>
        <p class="talkd">
            Erdős by <a target="_blank" href="//sdslabs.co">SDSLabs</a> is a competitive math learning platform, similar in spirit to <a href="https://projecteuler.net/">Project Euler</a>, albeit more feature-packed (support for holding competitions, has a social layer) and prettier.
            <a target="_blank" href="https://erdos.sdslabs.co"><img style="margin-top:10px;" src="/img/projects/erdos.jpg"></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/graf">graf</a></h2>
        <p class="talkd">
            graf plots pretty git contribution bar graphs in the terminal.
            <code>gem install graf</code> to install.
            <a target="_blank" href="https://github.com/abhshkdz/graf"><img style="margin-top:10px;" src="/img/projects/graf.gif"></a>
        </p>
    </div>
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/HackFlowy">HackFlowy</a></h2>
        <p class="talkd">
            Clone of <a href="//workflowy.com">WorkFlowy.com</a>, a beautiful, list-based note-taking website that has a 500-item monthly limit on the free tier :-(. This project is an open-source clone of WorkFlowy. "Make lists. Not war." :-)
            <a target="_blank" href="https://github.com/abhshkdz/HackFlowy"><img style="margin-top:40px;" src="/img/projects/hackflowy.png"></a>
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/AirMaps">AirMaps</a></h2>
        <p class="talkd">
            AirMaps was a fun hackathon project that lets users navigate through Google Earth with gestures and speech commands using a Kinect sensor. It was the <a target="_blank" href="https://blog.sdslabs.co/2014/02/code-fun-do">winning entry in Microsoft Code.Fun.Do</a>.
            <a target="_blank" href="https://github.com/abhshkdz/AirMaps"><img style="margin-top:10px;" src="/img/projects/airmaps.jpg"></a>
        </p>
    </div>
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/sdslabs/hackview">HackView</a></h2>
        <p class="talkd">
            Another fun hackathon-winning project built during Yahoo! HackU! 2012 that involves webRTC-based P2P video chat, and was faster than any other video chat provider (at the time, before Google launched Hangouts).
        </p>
    </div>
    <div class="col-sm-6">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/8tracks-downloader">8tracks-downloader</a></h2>
        <p class="talkd">
            Ugly-looking, but super-effective bash script for downloading entire playlists from 8tracks. (Still works as of 10/2016).
        </p>
    </div>
</div> -->

<script src="/js/jquery.min.js"></script>
<script type="text/javascript">
    $('ul:gt(0) li:gt(12)').hide();
    $('#read-more-button > a').click(function() {
        $('ul:gt(0) li:gt(12)').show();
        $('#read-more-button').hide();
    });
</script>

---
<!-- 
[1]: //mlp.cc.gatech.edu
[2]: ///www.cc.gatech.edu/~dbatra/
[3]: //www.cc.gatech.edu/~parikh/
[4]: //www.qbi.uq.edu.au/professor-geoffrey-goodhill
[5]: //researchers.uq.edu.au/researcher/2490
[6]: http://cns.qbi.uq.edu.au/
[7]: //developers.google.com/open-source/gsoc/
[8]: /posts/summer-of-code/
[9]: /posts/gsoc-reunion-2014/
[10]: //blog.sdslabs.co/2012/09/hacku
[11]: //blog.sdslabs.co/2014/02/code-fun-do
[12]: //www.facebook.com/SDSLabs/posts/527540147292475
[13]: /posts/deloitte-cctc-3/
[14]: /posts/google-india-community-summit/
[15]: //blog.sdslabs.co/2013/10/syntax-error-2013
[16]: //sdslabs.co/
[17]: //erdos.sdslabs.co/
[18]: //projecteuler.net/
[19]: //github.com/abhshkdz/neural-vqa
[20]: //github.com/abhshkdz/HackFlowy
[21]: //github.com/abhshkdz/graf
[22]: //github.com/abhshkdz
[23]: //twitter.com/abhshkdz
[24]: //instagram.com/abhshkdz
[25]: http://x.abhishekdas.com/
[26]: https://abhishekdas.com/vqa-hat/
[27]: http://arxiv.org/abs/1606.03556
[28]: https://www.newscientist.com/article/2095616-robot-eyes-and-humans-fix-on-different-things-to-decode-a-scene/
[29]: https://www.technologyreview.com/s/601819/ai-is-learning-to-see-the-world-but-not-the-way-humans-do/
[30]: http://www.theverge.com/2016/7/12/12158238/first-click-deep-learning-algorithmic-black-boxes
[31]: http://iitr.ac.in/
[32]: https://www.facebook.com/dhruv.batra.1253/posts/1783087161932290
[33]: https://drive.google.com/file/d/1nObeNzl-sTy8I5QN1Jv8wscebKLv-6RY/view?usp=sharing
[34]: http://aideadlin.es/
[35]: //github.com/abhshkdz/neural-vqa-attention
[36]: https://snapresearchfellowship.splashthat.com/
[37]: https://www.youtube.com/watch?v=R4hugGnNr7s
[38]: https://www.youtube.com/watch?v=I9OlorMh7wU
[39]: https://adoberesearch.ctlprojects.com/fellowship/previous-fellowship-award-winners/
[40]: https://embodiedqa.org/
[41]: https://youtu.be/KAlGWMJnWyc?t=26m56s
[42]: https://2018gputechconf.smarteventscloud.com/connect/sessionDetail.ww?SESSION_ID=152715
[43]: https://www.ic.gatech.edu/news/600684/three-ic-students-earn-snap-research-awards
[44]: https://www.ic.gatech.edu/news/601084/new-research-fellowships-offer-two-students-funding-access-adobes-creative-cloud
[45]: https://github.com/facebookresearch/House3D
[46]: https://gkioxari.github.io/
[47]: https://research.fb.com/people/parikh-devi/
[48]: https://research.fb.com/people/batra-dhruv/
[49]: https://lvatutorial.github.io/
[50]: http://acl2018.org/tutorials/#connecting-language-and-vis
[51]: http://visualqa.org/workshop.html
[52]: http://on-demand.gputechconf.com/gtc/2018/video/S8582/
[53]: https://visualdialog.org/challenge/2018
[54]: https://youtu.be/gz2VoDrvX-A?t=1h19m58s
[55]: https://research.fb.com/people/rabbat-mike/
[56]: https://www.cs.mcgill.ca/~jpineau/
[57]: https://visualdialog.org/challenge/2018#winners
[58]: https://www.youtube.com/watch?v=xoHvho-YRgs&t=7330
[fb-fellow-page]: https://research.fb.com/announcing-the-2019-facebook-fellows-and-emerging-scholars/
[joelle-corl18-talk-mention]: https://www.youtube.com/watch?v=FSsEqEJKo8A&t=3497
[visdial-challenge-2]: https://visualdialog.org/challenge/2019
[ic-gt-article]: https://www.ic.gatech.edu/news/617061/see-and-say-abhishek-das-working-provide-crucial-communication-tools-intelligent-agents
[caliper]: https://caliper.ai
[felix-hill]: https://fh295.github.io
[laura-rimell]: http://www.rimell.cc/laura/
[stephen-clark]: https://sites.google.com/site/stephenclark609/
[andrej-karpathy]: https://karpathy.ai/
[vigil19]: https://vigilworkshop.github.io/2019
[tarmac-icml-talk]: https://www.facebook.com/icml.imls/videos/444326646299556/
[mastodon]: https://mastodon.social/web/accounts/1011404
[conquerearth]: https://conquer.earth/abhshkdz
[qa-probing-icml20-talk]: https://slideslive.com/38928261/probing-emergent-semantics-in-predictive-agents-via-question-answering
[vigil20]: https://vigilworkshop.github.io
[ocp]: https://opencatalystproject.org
[ocp-cnbc]: https://www.cnbc.com/2020/10/14/facebook-to-use-ai-in-bid-to-improve-renewable-energy-storage.html
[ocp-engadget]: https://engadget.com/facebook-deploys-its-ai-to-find-green-energy-storage-solutions-130041147.html
[ocp-fortune]: https://fortune.com/2020/10/14/facebook-ai-open-catalyst-dataset-chemistry-renewable-energy/
[ocp-venturebeat]: https://venturebeat.com/2020/10/14/facebook-and-carnegie-mellon-launch-project-to-discover-better-ways-to-store-renewable-energy/
[aipaygrad.es]: https://aipaygrad.es
[sigma-xi-thesis-award]: https://cpb-us-w2.wpmucdn.com/sites.gatech.edu/dist/0/283/files/2021/03/2021-Sigma-Xi-Research-Award-Winners.final_.pdf
[coc-dissertation-award]: https://sites.gatech.edu/gtcomputingawards2021/graduate-student-awards/
[thesis-pdf]: https://drive.google.com/file/u/2/d/1b2Gonazl1Os0eLPV9frkucEqSuRroEvD/view?usp=sharing
[aaai-dissertation-award]: https://aaai.org/Awards/dissertation-award.php -->
